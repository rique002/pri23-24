{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRI Project\n",
    "\n",
    "## Data Processing\n",
    "\n",
    "Write something about the project here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write something about data 1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data 1 csv and drop the unnecessary data\n",
    "jobs_data_1 = pd.read_csv('data/allJobs.csv')\n",
    "jobs_data_1.drop(columns=['Date-Posted'],inplace=True)\n",
    "jobs_data_1.dropna()\n",
    "jobs_data_1.rename(columns = {'Company':'company','Job-Title':'title','Job-Type':'work-type','Salary':'salary','Location':'location','Description':'description'}, inplace = True)\n",
    "jobs_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write something about data 2 hera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data 2 csv and drop the unnecessary data\n",
    "jobs_data_2 = pd.read_csv('data/job_postings.csv')\n",
    "jobs_data_2.drop(columns=['max_salary','min_salary','job_id','expiry','closed_time','applies','currency','compensation_type','original_listed_time','remote_allowed','views','job_posting_url','application_url','application_type','formatted_experience_level','skills_desc','listed_time','posting_domain','sponsored','work_type'],inplace=True)\n",
    "jobs_data_2.rename(columns = {'med_salary':'salary','formatted_work_type':'work-type'}, inplace = True)\n",
    "jobs_data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write something about the salary in jobs_data_1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_salary_and_pay_period(salary_string):\n",
    "    salary_string = str(salary_string)\n",
    "    if 'a year' in salary_string:\n",
    "        pay_period = 'YEARLY'\n",
    "        match = re.search(r'\\$([\\d,.]+[kK]?)-?([\\d,.]+[kK]?)?', salary_string)\n",
    "        if match:\n",
    "            low_value = match.group(1).replace('.', '').replace(',', '').replace('k', '000').replace('K', '000')\n",
    "            high_value = match.group(2).replace('.', '').replace(',', '').replace('k', '000').replace('K', '000') if match.group(2) else low_value\n",
    "            salary = (float(low_value) + float(high_value)) / 2\n",
    "        else:\n",
    "            match = re.search(r'\\$([\\d,.]+[kK]?)', salary_string)\n",
    "            if match:\n",
    "                salary = float(match.group(1).replace('.', '').replace(',', '').replace('k', '000').replace('K', '000'))\n",
    "            else:\n",
    "                salary = None\n",
    "    elif 'a month' in salary_string:\n",
    "        pay_period = 'MONTHLY'\n",
    "        match = re.findall(r'\\$([\\d,.]+[kK]?)', salary_string)\n",
    "        if match:\n",
    "            salary_range = [(float(m.replace('.', '').replace(',', '').replace('k', '000').replace('K', '000'))) for m in match]\n",
    "            salary = sum(salary_range) / len(salary_range)\n",
    "        else:\n",
    "            salary = None\n",
    "    elif 'an hour' in salary_string:\n",
    "        pay_period = 'HOURLY'\n",
    "        match = re.search(r'\\$([\\d,.]+[kK]?)', salary_string)\n",
    "        if match:\n",
    "            salary = float(match.group(1).replace('.', '').replace(',', '').replace('k', '000').replace('K', '000'))\n",
    "        else:\n",
    "            salary = None\n",
    "    else:\n",
    "        pay_period = None\n",
    "        salary = None\n",
    "    return salary, pay_period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... apply the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the salary collumn into salary and pay period\n",
    "jobs_data_1['salary'], jobs_data_1['pay_period'] = zip(*jobs_data_1['salary'].apply(extract_salary_and_pay_period))\n",
    "# Remove outliers \n",
    "jobs_data_1 = pd.concat([jobs_data_1[jobs_data_1['salary'] < 1000000], jobs_data_1[jobs_data_1['salary'].isnull()]])\n",
    "jobs_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. add company data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe for company data\n",
    "company_data_1 = jobs_data_1[\"company\"].drop_duplicates().reset_index(drop=True).reset_index()\n",
    "company_data_1.columns = ['company_id', 'company']\n",
    "company_data_1['company_id'] = company_data_1['company_id'] + 3700152515\n",
    "\n",
    "jobs_data_1 = pd.merge(jobs_data_1, company_data_1, on='company', how='left')\n",
    "jobs_data_1.drop(columns=['company'],inplace=True)\n",
    "jobs_data_1['work-type'].replace({'Contractor': 'Contract'}, inplace=True)\n",
    "\n",
    "company_data_2 = pd.read_csv('data/companies.csv')\n",
    "company_data_2.drop(columns=['description',\t'company_size',\t'state'\t,'country'\t,'city',\t'zip_code',\t'address',\t'url'],inplace=True)\n",
    "company_data_2.rename(columns = {'name':'company'}, inplace = True)\n",
    "\n",
    "company_data = pd.concat([company_data_1,company_data_2])\n",
    "company_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes\n",
    "jobs_data = pd.concat([jobs_data_1, jobs_data_2], ignore_index=True)\n",
    "jobs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... write something about country data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read countries csv and drop the unnecessary data\n",
    "countries_data = pd.read_csv('data/countries.csv')\n",
    "countries_data = countries_data[['name','alpha-2','region','sub-region']]\n",
    "countries_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... extract city and country data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_city_and_country(location_string, country_list):\n",
    "    splited_location = location_string.split(',')\n",
    "    first_word = splited_location[0]\n",
    "    last_word = splited_location[-1][1:]\n",
    "    country_names = country_list['name'].values\n",
    "    country_alpha = country_list['alpha-2'].values\n",
    "\n",
    "    if first_word in country_names:\n",
    "        region = country_list[country_list['name'] == first_word]['region'].iloc[0]\n",
    "        return None, first_word, region\n",
    "    elif first_word in country_alpha:\n",
    "        country = country_list[country_list['alpha-2'] == first_word]['name'].iloc[0]\n",
    "        region = country_list[country_list['alpha-2'] == first_word]['region'].iloc[0]\n",
    "        return None, country, region\n",
    "    elif last_word in country_names:\n",
    "        region = country_list[country_list['name'] == last_word]['region'].iloc[0]\n",
    "        return first_word, last_word, region\n",
    "    elif last_word in country_alpha:\n",
    "        country = country_list[country_list['alpha-2'] == last_word]['name'].iloc[0]\n",
    "        region = country_list[country_list['alpha-2'] == last_word]['region'].iloc[0]\n",
    "        return first_word, country, region\n",
    "    elif len(splited_location) > 0:\n",
    "        return first_word, 'United States of America', 'Americas'\n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... aply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create location table\n",
    "jobs_location_data = pd.DataFrame(jobs_data['location'], columns=['location'])\n",
    "jobs_location_data['city'], jobs_location_data['country'], jobs_location_data['region'] = zip(*jobs_location_data['location'].apply(lambda x: extract_city_and_country(x, countries_data)))\n",
    "jobs_location_data.drop_duplicates(inplace=True)\n",
    "jobs_location_data.reset_index(drop=True, inplace=True)\n",
    "jobs_location_data.insert(0, 'location_id', range(1, 1 + len(jobs_location_data)))\n",
    "\n",
    "jobs_data = pd.merge(jobs_data, jobs_location_data[['location', 'location_id']], on='location', how='left')\n",
    "jobs_data.drop(columns=['location'], inplace=True)\n",
    "\n",
    "jobs_location_data.drop(columns=['location'], inplace=True)\n",
    "jobs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... graphic 8K HDR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jobs_data['pay_period'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Pay-Period')\n",
    "plt.ylabel('Job postings')\n",
    "plt.title('Job postings by Pay-Period')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_data_final_copy = jobs_data.copy().reset_index()\n",
    "\n",
    "for idx ,job in jobs_data_final_copy.iterrows():\n",
    "    if job[\"pay_period\"] == \"MONTHLY\":\n",
    "        jobs_data_final_copy.at[idx ,\"salary\"] *= 12\n",
    "    elif job[\"pay_period\"] == \"HOURLY\":\n",
    "        jobs_data_final_copy.at[idx ,\"salary\"] *= 1810\n",
    "\n",
    "average_salary_by_work_type = jobs_data_final_copy.groupby('work-type')['salary'].mean().reset_index()\n",
    "plt.figure(figsize=(10 ,6))  \n",
    "plt.bar(average_salary_by_work_type[\"work-type\"] ,average_salary_by_work_type[\"salary\"] ,color='skyblue')\n",
    "plt.xlabel('Work Type')\n",
    "plt.ylabel('Average Salary')\n",
    "plt.title('Average Salary by Work Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_data_final_copy = pd.merge(jobs_data_final_copy, jobs_location_data, on='location_id', how='left')\n",
    "average_salary_by_region = jobs_data_final_copy.groupby('region')['salary'].mean().reset_index()\n",
    "plt.figure(figsize=(10 ,6))\n",
    "plt.bar(average_salary_by_region[\"region\"] ,average_salary_by_region[\"salary\"] ,color='skyblue')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Average Salary')\n",
    "plt.title('Average Salary by Region')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_data_final_copy['region'].value_counts().plot(kind='pie',explode=(.1, 0, 0, 0,0),shadow=True,autopct='%1.1f%%',ylabel=\"\",title=\"Job postings by World Region\")\n",
    "jobs_data_final_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize docker container\n",
    "!powershell -command \"docker compose down\"\n",
    "!powershell -command \"docker compose up -d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until solr is ready\n",
    "import requests\n",
    "import time\n",
    "\n",
    "url = 'http://localhost:8983/solr'\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            print('Solr is ready.')\n",
    "            break\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Solr is not ready yet, waiting...')\n",
    "        time.sleep(2)  # wait for 10 seconds before trying again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path = 'solr/jobs_schema.json'\n",
    "\n",
    "with open(schema_path, 'rb') as f:\n",
    "    schema_data = f.read()\n",
    "\n",
    "headers = {'Content-type': 'application/json'}\n",
    "\n",
    "r = requests.post(url, data=schema_data, headers=headers)\n",
    "\n",
    "if r.status_code == 200:\n",
    "    print('Schema updated successfully')\n",
    "else:\n",
    "    print('Error updating schema')\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysolr\n",
    "\n",
    "solr = pysolr.Solr('http://localhost:8983/solr/mycore', timeout=10)\n",
    "\n",
    "all_data = pd.merge(jobs_data, jobs_location_data, on='location_id', how='left')\n",
    "all_data = pd.merge(all_data, company_data, on='company_id', how='left')\n",
    "all_data.drop(columns=['location_id','company_id'],inplace=True)\n",
    "all_data_doc = all_data.to_dict(orient='records')\n",
    "\n",
    "chunk_size = 2000  # adjust this value based on your needs\n",
    "\n",
    "# Split data into chunks and add them to Solr\n",
    "for i in range(0, len(all_data_doc), chunk_size):\n",
    "    chunk = all_data_doc[i:i+chunk_size]\n",
    "    solr.add(chunk)\n",
    "    solr.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1 - Administration jobs in education establishments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple query\n",
    "sq1 = solr.search(q='(company:university OR company:school OR company:college) AND (title:administration OR description:administration)', \n",
    "                sort='score desc', rows='10')\n",
    "\n",
    "for hit in sq1:\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting query\n",
    "bq1 = solr.search(q='(company:university OR company:school OR company:college) AND (title:administration^4 OR description:administration)', \n",
    "                sort='score desc', rows='10')\n",
    "for hit in bq1:\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2 - Jobs as part of a team in medical field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple query\n",
    "sq2 = solr.search(q='(title:medical OR description:medical) AND (title:team OR description:team)', \n",
    "                sort='score desc', rows='10')\n",
    "\n",
    "for hit in sq2:\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting query\n",
    "bq2 = solr.search(q='(title:medical^10 OR description:medical^5) AND (title:team OR description:team)', \n",
    "                sort='score desc', rows='10')\n",
    "\n",
    "for hit in bq2:\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3 - Data engineering jobs with 3 year experience required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple query\n",
    "sq3 = solr.search(q='(title:data description:data title:engineering description:engineering) AND description:\"3 year experience\"', \n",
    "                sort='score desc', rows='10')\n",
    "\n",
    "for hit in sq3:\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting query\n",
    "bq3 = solr.search(q=' (title:data^5 description:data title:engineering^5 description:engineering) AND description:\"3 year experience\"~10', \n",
    "                sort='score desc', rows='10')\n",
    "\n",
    "for hit in bq3:\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4 - Non-contract jobs, paid by hour, not computation related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple query\n",
    "sq4 = solr.search(q='(-work-type:Contract AND pay_period:HOURLY AND -title:computation AND -description:computation)', \n",
    "                sort='score desc', rows='10')\n",
    "\n",
    "for hit in sq4:\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting query\n",
    "sq4 = solr.search(q='(-work-type:Contract AND pay_period:HOURLY^5 AND -title:computation AND -description:computation)', \n",
    "                sort='score desc', rows='10')\n",
    "\n",
    "for hit in sq4:\n",
    "    print(hit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
